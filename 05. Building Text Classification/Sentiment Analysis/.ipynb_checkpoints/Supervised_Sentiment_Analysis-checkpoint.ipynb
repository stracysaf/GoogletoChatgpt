{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7abbc6cf-a3cc-4a46-9a7d-99aee2081efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66e204c2-0ea1-4e66-a871-594187023d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('movie_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7755598-f7c7-442d-8b31-6f9fe6bcef0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    25000\n",
       "negative    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dbf9125-cabe-4d2c-9d89-2237e8fa3d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    1501\n",
       "negative    1499\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.sample(3000).sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3d18c3e-1ab8-4d93-8866-cfde97c40bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_sample = reviews.sample(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4db5386-6ffd-4c8b-9167-14da13c67372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "negative    1502\n",
       "positive    1498\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_sample.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0202ecb6-b54d-48ae-853c-56035900cdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_sample = reviews_sample.reset_index().drop(columns = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2c63e83-df7e-436a-a246-36780f38cdad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>After reading the book, Heart of Darkness, the...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Title: Zombie 3 (1988) &lt;br /&gt;&lt;br /&gt;Directors: ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I got all excited when I saw the ads for this ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This film is basically two hours of Dafoe's ch...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bergman´s tale about how the hell of the war c...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>I've never been impressed by JD anyway, and Fi...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>I just finished watching this movie and am dis...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>I just read the comments of TomReynolds2004 an...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>I watched this film in a very strange way -- I...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>This is Burt Reynolds'\"Citizen Kane\".Tragicall...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review sentiment\n",
       "0     After reading the book, Heart of Darkness, the...  negative\n",
       "1     Title: Zombie 3 (1988) <br /><br />Directors: ...  negative\n",
       "2     I got all excited when I saw the ads for this ...  negative\n",
       "3     This film is basically two hours of Dafoe's ch...  negative\n",
       "4     Bergman´s tale about how the hell of the war c...  positive\n",
       "...                                                 ...       ...\n",
       "2995  I've never been impressed by JD anyway, and Fi...  negative\n",
       "2996  I just finished watching this movie and am dis...  negative\n",
       "2997  I just read the comments of TomReynolds2004 an...  positive\n",
       "2998  I watched this film in a very strange way -- I...  positive\n",
       "2999  This is Burt Reynolds'\"Citizen Kane\".Tragicall...  positive\n",
       "\n",
       "[3000 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbeb587a-8bb3-4199-a2ae-89c7d3b2e96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "\n",
    "def strip_html(doc):\n",
    "    soup = BeautifulSoup(doc,\"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    return text\n",
    "    \n",
    "\n",
    "def normalize_document(doc):\n",
    "    doc = strip_html(doc)\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8900c21-d343-4208-8199-92f38739ef4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_sample['review_sample'] = reviews_sample['review'].apply(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c430c765-e674-4029-8ae7-60c617536c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>After reading the book, Heart of Darkness, the...</td>\n",
       "      <td>negative</td>\n",
       "      <td>reading book heart darkness movie justice movi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Title: Zombie 3 (1988) &lt;br /&gt;&lt;br /&gt;Directors: ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>title zombie directors mostly lucio fulci also...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I got all excited when I saw the ads for this ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>got excited saw ads movie recently read book r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This film is basically two hours of Dafoe's ch...</td>\n",
       "      <td>negative</td>\n",
       "      <td>film basically two hours dafoes character drin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bergman´s tale about how the hell of the war c...</td>\n",
       "      <td>positive</td>\n",
       "      <td>bergmans tale hell war drive sensible couple m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>I've never been impressed by JD anyway, and Fi...</td>\n",
       "      <td>negative</td>\n",
       "      <td>ive never impressed jd anyway final justice ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>I just finished watching this movie and am dis...</td>\n",
       "      <td>negative</td>\n",
       "      <td>finished watching movie disappointed say didnt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>I just read the comments of TomReynolds2004 an...</td>\n",
       "      <td>positive</td>\n",
       "      <td>read comments tomreynolds feel jump understand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>I watched this film in a very strange way -- I...</td>\n",
       "      <td>positive</td>\n",
       "      <td>watched film strange way put netflix list coul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>This is Burt Reynolds'\"Citizen Kane\".Tragicall...</td>\n",
       "      <td>positive</td>\n",
       "      <td>burt reynoldscitizen kanetragically nothing el...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review sentiment  \\\n",
       "0     After reading the book, Heart of Darkness, the...  negative   \n",
       "1     Title: Zombie 3 (1988) <br /><br />Directors: ...  negative   \n",
       "2     I got all excited when I saw the ads for this ...  negative   \n",
       "3     This film is basically two hours of Dafoe's ch...  negative   \n",
       "4     Bergman´s tale about how the hell of the war c...  positive   \n",
       "...                                                 ...       ...   \n",
       "2995  I've never been impressed by JD anyway, and Fi...  negative   \n",
       "2996  I just finished watching this movie and am dis...  negative   \n",
       "2997  I just read the comments of TomReynolds2004 an...  positive   \n",
       "2998  I watched this film in a very strange way -- I...  positive   \n",
       "2999  This is Burt Reynolds'\"Citizen Kane\".Tragicall...  positive   \n",
       "\n",
       "                                          review_sample  \n",
       "0     reading book heart darkness movie justice movi...  \n",
       "1     title zombie directors mostly lucio fulci also...  \n",
       "2     got excited saw ads movie recently read book r...  \n",
       "3     film basically two hours dafoes character drin...  \n",
       "4     bergmans tale hell war drive sensible couple m...  \n",
       "...                                                 ...  \n",
       "2995  ive never impressed jd anyway final justice ha...  \n",
       "2996  finished watching movie disappointed say didnt...  \n",
       "2997  read comments tomreynolds feel jump understand...  \n",
       "2998  watched film strange way put netflix list coul...  \n",
       "2999  burt reynoldscitizen kanetragically nothing el...  \n",
       "\n",
       "[3000 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7bf9d81-2512-411d-a202-406b900d6e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = reviews_sample['review_sample']\n",
    "y = reviews_sample['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb18241c-0819-4e89-a76b-abf133529b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(X,y, test_size=0.33, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a69efcd-990e-46c4-ada3-61ef24646d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2010,), (990,), (2010,), (990,))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape, test_X.shape, train_y.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79df04d-943f-4d02-9176-4d697a882c12",
   "metadata": {},
   "source": [
    "# Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b570692b-6a85-441e-b124-3ff26c17f438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e42192de-e4bb-493f-9001-d37d074eb8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fefb15d8-f0bb-462b-ab4e-e27bcc8bc7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_transfored_train_X = cv.fit_transform(train_X)\n",
    "cv_transfored_test_X = cv.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f6305179-b7ea-4222-830a-cbf9c366271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4aa1b46d-0b7e-4ab9-801f-d8e67d8e5d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(cv_transfored_train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d59a9fa6-2d5e-41d6-8318-1e636336dd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = model.predict(cv_transfored_train_X)\n",
    "test_pred = model.predict(cv_transfored_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bfbb2a94-7c88-4d3f-a26d-3758b649f1e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1009,    0],\n",
       "       [   0, 1001]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(train_y, train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c5bace65-9433-4751-8c5c-2c7354cbd5db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[401,  92],\n",
       "       [ 82, 415]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_y, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6d2270a7-4af8-4699-a5e7-6e8b9d10adac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy :  1.0\n",
      "Test Accuracy :  0.8242424242424242\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Accuracy : \",accuracy_score(train_y, train_pred))\n",
    "print(\"Test Accuracy : \",accuracy_score(test_y, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6490be7d-582e-4429-bcd6-05a0c70df0dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b6c809-5aaa-4528-8b06-1490576029c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90df7a6-a0d4-4db0-bf31-8744e4fd2bda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74f24b8a-1d2e-4b88-ad72-7e9ab796cfee",
   "metadata": {},
   "source": [
    "# Bag of Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1945bc9e-0109-451a-87c3-85729ecbe046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "02c5658a-eb10-425e-b2fc-7d276d310c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "00278bbc-f9d8-4dcb-9f81-1db39c8d072c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_transfored_train_X = cv.fit_transform(train_X)\n",
    "cv_transfored_test_X = cv.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "783423ef-7d74-47fd-a6c0-1f7e1130c2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e4801568-4616-4161-838c-2bde912d945e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(cv_transfored_train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9d66fde2-528c-4afb-a42b-039b9b40dd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = model.predict(cv_transfored_train_X)\n",
    "test_pred = model.predict(cv_transfored_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "92de8c53-8e62-46d2-8aac-00b18ef30dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1009,    0],\n",
       "       [   0, 1001]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(train_y, train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b8d695a8-965f-4154-bc82-96e896e7769f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[400,  93],\n",
       "       [ 82, 415]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_y, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "808cdd04-efba-498b-b083-e7c12edcb15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy :  1.0\n",
      "Test Accuracy :  0.8232323232323232\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Accuracy : \",accuracy_score(train_y, train_pred))\n",
    "print(\"Test Accuracy : \",accuracy_score(test_y, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d48890e-8aaa-498f-8ac4-e8497872a90c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d249e5b-74e8-49e3-ab7e-62322bac1152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5875736-8a2a-4e89-9e90-176d1e7feb40",
   "metadata": {},
   "source": [
    "# TF - IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5ae8b6f2-c22f-496c-a7fc-94d008b177c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "447732b4-ce5b-47c0-a017-5ef8fa2acdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "97eb567e-adb8-4b7a-b761-8a20652dbb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_transfored_train_X = tf.fit_transform(train_X)\n",
    "tf_transfored_test_X = tf.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fa946c18-6b98-48d3-8986-5ec9bf6d8042",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0f4e341f-7167-46a5-87ab-862ac9145af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tf_transfored_train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fbfd6577-bf98-439c-8741-84c53252a8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = model.predict(tf_transfored_train_X)\n",
    "test_pred = model.predict(tf_transfored_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "75626dcd-7b1c-4ee7-88d1-e1834c8510e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[980,  29],\n",
       "       [ 24, 977]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(train_y, train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8432c038-b2f1-4053-9357-5f2cebf69459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[404,  89],\n",
       "       [ 74, 423]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_y, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2ff0eb44-4c7b-48f3-8efa-b5551d169dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy :  0.9736318407960199\n",
      "Test Accuracy :  0.8353535353535354\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Accuracy : \",accuracy_score(train_y, train_pred))\n",
    "print(\"Test Accuracy : \",accuracy_score(test_y, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a39772d-4262-4f70-97bc-12993958c0f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acdccd0a-372f-451d-831d-602f8eeb63a0",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263bacf2-ae5c-44c8-9d69-6ebef0d9fc62",
   "metadata": {},
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e74557c3-6425-48b1-bacb-9fb1db9ae108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings into a dictionary\n",
    "def load_embeddings(file_path):\n",
    "    embeddings = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "glove_embeddings_path = '../Scriptures/glove.6B.300d.txt'  # Adjust the path to your downloaded GloVe file\n",
    "wv = load_embeddings(glove_embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1eff915a-9fcc-4301-9197-d469da0761cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "    \n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return feature_vector\n",
    "\n",
    "\n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.keys())\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "07451e1f-044c-4ee7-8dab-5d3bc40d7fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_corpus = np.vectorize(normalize_document)\n",
    "train_norm_corpus = normalize_corpus(train_X)\n",
    "train_tokenized_corpus = [nltk.word_tokenize(doc) for doc in train_norm_corpus]\n",
    "\n",
    "test_norm_corpus = normalize_corpus(test_X)\n",
    "test_tokenized_corpus = [nltk.word_tokenize(doc) for doc in test_norm_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9f07b3a8-abc1-4beb-ae00-dfa9eb89faac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get document level embeddings\n",
    "feature_size = 300\n",
    "train_features_X = averaged_word_vectorizer(corpus=train_tokenized_corpus, model=wv,\n",
    "                                             num_features=feature_size)\n",
    "\n",
    "test_features_X = averaged_word_vectorizer(corpus=test_tokenized_corpus, model=wv,\n",
    "                                             num_features=feature_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e4c69efb-340b-4e5a-a855-a400017f229b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.095210</td>\n",
       "      <td>0.074208</td>\n",
       "      <td>-0.075189</td>\n",
       "      <td>-0.030519</td>\n",
       "      <td>-0.039498</td>\n",
       "      <td>0.030736</td>\n",
       "      <td>-0.063679</td>\n",
       "      <td>0.019690</td>\n",
       "      <td>-0.030630</td>\n",
       "      <td>-0.982443</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033514</td>\n",
       "      <td>-0.130089</td>\n",
       "      <td>-0.034337</td>\n",
       "      <td>0.004901</td>\n",
       "      <td>-0.032685</td>\n",
       "      <td>-0.012092</td>\n",
       "      <td>0.049251</td>\n",
       "      <td>-0.013650</td>\n",
       "      <td>-0.049144</td>\n",
       "      <td>0.040632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.013725</td>\n",
       "      <td>0.034086</td>\n",
       "      <td>-0.034454</td>\n",
       "      <td>-0.092877</td>\n",
       "      <td>0.016656</td>\n",
       "      <td>-0.032830</td>\n",
       "      <td>0.007448</td>\n",
       "      <td>0.008423</td>\n",
       "      <td>0.088812</td>\n",
       "      <td>-1.173755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010705</td>\n",
       "      <td>-0.071519</td>\n",
       "      <td>-0.028189</td>\n",
       "      <td>-0.021560</td>\n",
       "      <td>0.020954</td>\n",
       "      <td>-0.086438</td>\n",
       "      <td>-0.016503</td>\n",
       "      <td>-0.022217</td>\n",
       "      <td>-0.050941</td>\n",
       "      <td>0.033541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.055449</td>\n",
       "      <td>0.011165</td>\n",
       "      <td>-0.027353</td>\n",
       "      <td>-0.026629</td>\n",
       "      <td>0.029692</td>\n",
       "      <td>0.039979</td>\n",
       "      <td>0.003559</td>\n",
       "      <td>0.022882</td>\n",
       "      <td>-0.012319</td>\n",
       "      <td>-1.174624</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045856</td>\n",
       "      <td>-0.084983</td>\n",
       "      <td>-0.020713</td>\n",
       "      <td>0.038621</td>\n",
       "      <td>0.006540</td>\n",
       "      <td>0.090790</td>\n",
       "      <td>-0.031528</td>\n",
       "      <td>-0.063819</td>\n",
       "      <td>-0.022391</td>\n",
       "      <td>0.042568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.072958</td>\n",
       "      <td>0.083566</td>\n",
       "      <td>-0.049090</td>\n",
       "      <td>-0.077937</td>\n",
       "      <td>-0.047330</td>\n",
       "      <td>0.075804</td>\n",
       "      <td>-0.045134</td>\n",
       "      <td>-0.047141</td>\n",
       "      <td>0.036042</td>\n",
       "      <td>-0.992552</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046525</td>\n",
       "      <td>-0.071441</td>\n",
       "      <td>0.017279</td>\n",
       "      <td>0.046466</td>\n",
       "      <td>-0.054474</td>\n",
       "      <td>-0.075757</td>\n",
       "      <td>0.024841</td>\n",
       "      <td>-0.043944</td>\n",
       "      <td>0.007172</td>\n",
       "      <td>0.068454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.042490</td>\n",
       "      <td>0.055707</td>\n",
       "      <td>-0.027435</td>\n",
       "      <td>-0.052708</td>\n",
       "      <td>0.088608</td>\n",
       "      <td>-0.030552</td>\n",
       "      <td>0.029926</td>\n",
       "      <td>0.010903</td>\n",
       "      <td>0.030163</td>\n",
       "      <td>-0.944765</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028950</td>\n",
       "      <td>-0.099857</td>\n",
       "      <td>0.046651</td>\n",
       "      <td>-0.017479</td>\n",
       "      <td>0.027504</td>\n",
       "      <td>-0.106758</td>\n",
       "      <td>-0.026807</td>\n",
       "      <td>-0.033834</td>\n",
       "      <td>-0.049099</td>\n",
       "      <td>0.047046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>-0.092523</td>\n",
       "      <td>0.081752</td>\n",
       "      <td>-0.063385</td>\n",
       "      <td>-0.033966</td>\n",
       "      <td>0.056871</td>\n",
       "      <td>0.034229</td>\n",
       "      <td>-0.009610</td>\n",
       "      <td>0.025635</td>\n",
       "      <td>0.092939</td>\n",
       "      <td>-1.273092</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016819</td>\n",
       "      <td>-0.139673</td>\n",
       "      <td>-0.081753</td>\n",
       "      <td>-0.032949</td>\n",
       "      <td>0.077111</td>\n",
       "      <td>-0.101313</td>\n",
       "      <td>-0.003604</td>\n",
       "      <td>-0.073269</td>\n",
       "      <td>-0.119029</td>\n",
       "      <td>0.070678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>-0.034708</td>\n",
       "      <td>0.105868</td>\n",
       "      <td>-0.031159</td>\n",
       "      <td>-0.018991</td>\n",
       "      <td>-0.004858</td>\n",
       "      <td>-0.095991</td>\n",
       "      <td>-0.065348</td>\n",
       "      <td>-0.039109</td>\n",
       "      <td>-0.028009</td>\n",
       "      <td>-0.958242</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044165</td>\n",
       "      <td>-0.089371</td>\n",
       "      <td>-0.016946</td>\n",
       "      <td>-0.028961</td>\n",
       "      <td>-0.026986</td>\n",
       "      <td>-0.113882</td>\n",
       "      <td>0.053097</td>\n",
       "      <td>-0.047088</td>\n",
       "      <td>-0.024108</td>\n",
       "      <td>0.029061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>-0.052781</td>\n",
       "      <td>0.045932</td>\n",
       "      <td>-0.024633</td>\n",
       "      <td>-0.101543</td>\n",
       "      <td>-0.003141</td>\n",
       "      <td>0.011376</td>\n",
       "      <td>-0.027632</td>\n",
       "      <td>0.015630</td>\n",
       "      <td>0.041839</td>\n",
       "      <td>-1.078481</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033619</td>\n",
       "      <td>-0.082491</td>\n",
       "      <td>-0.015605</td>\n",
       "      <td>-0.047796</td>\n",
       "      <td>-0.018306</td>\n",
       "      <td>0.026556</td>\n",
       "      <td>-0.104593</td>\n",
       "      <td>-0.058046</td>\n",
       "      <td>-0.049240</td>\n",
       "      <td>0.011466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>-0.062242</td>\n",
       "      <td>0.077842</td>\n",
       "      <td>0.003939</td>\n",
       "      <td>-0.034427</td>\n",
       "      <td>0.034741</td>\n",
       "      <td>0.093695</td>\n",
       "      <td>0.013632</td>\n",
       "      <td>0.045812</td>\n",
       "      <td>0.124574</td>\n",
       "      <td>-0.915449</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057947</td>\n",
       "      <td>-0.107110</td>\n",
       "      <td>-0.059751</td>\n",
       "      <td>0.042056</td>\n",
       "      <td>0.046700</td>\n",
       "      <td>-0.087650</td>\n",
       "      <td>0.009637</td>\n",
       "      <td>-0.017887</td>\n",
       "      <td>-0.061365</td>\n",
       "      <td>0.116531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>-0.047079</td>\n",
       "      <td>0.045922</td>\n",
       "      <td>-0.017486</td>\n",
       "      <td>-0.038547</td>\n",
       "      <td>0.008442</td>\n",
       "      <td>0.051992</td>\n",
       "      <td>-0.021564</td>\n",
       "      <td>0.027495</td>\n",
       "      <td>0.021102</td>\n",
       "      <td>-1.011296</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004887</td>\n",
       "      <td>-0.040088</td>\n",
       "      <td>0.006978</td>\n",
       "      <td>0.005945</td>\n",
       "      <td>0.031613</td>\n",
       "      <td>-0.037447</td>\n",
       "      <td>-0.002787</td>\n",
       "      <td>-0.014828</td>\n",
       "      <td>-0.054768</td>\n",
       "      <td>0.074454</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2010 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0    -0.095210  0.074208 -0.075189 -0.030519 -0.039498  0.030736 -0.063679   \n",
       "1    -0.013725  0.034086 -0.034454 -0.092877  0.016656 -0.032830  0.007448   \n",
       "2    -0.055449  0.011165 -0.027353 -0.026629  0.029692  0.039979  0.003559   \n",
       "3    -0.072958  0.083566 -0.049090 -0.077937 -0.047330  0.075804 -0.045134   \n",
       "4    -0.042490  0.055707 -0.027435 -0.052708  0.088608 -0.030552  0.029926   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2005 -0.092523  0.081752 -0.063385 -0.033966  0.056871  0.034229 -0.009610   \n",
       "2006 -0.034708  0.105868 -0.031159 -0.018991 -0.004858 -0.095991 -0.065348   \n",
       "2007 -0.052781  0.045932 -0.024633 -0.101543 -0.003141  0.011376 -0.027632   \n",
       "2008 -0.062242  0.077842  0.003939 -0.034427  0.034741  0.093695  0.013632   \n",
       "2009 -0.047079  0.045922 -0.017486 -0.038547  0.008442  0.051992 -0.021564   \n",
       "\n",
       "           7         8         9    ...       290       291       292  \\\n",
       "0     0.019690 -0.030630 -0.982443  ... -0.033514 -0.130089 -0.034337   \n",
       "1     0.008423  0.088812 -1.173755  ...  0.010705 -0.071519 -0.028189   \n",
       "2     0.022882 -0.012319 -1.174624  ... -0.045856 -0.084983 -0.020713   \n",
       "3    -0.047141  0.036042 -0.992552  ...  0.046525 -0.071441  0.017279   \n",
       "4     0.010903  0.030163 -0.944765  ... -0.028950 -0.099857  0.046651   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2005  0.025635  0.092939 -1.273092  ... -0.016819 -0.139673 -0.081753   \n",
       "2006 -0.039109 -0.028009 -0.958242  ... -0.044165 -0.089371 -0.016946   \n",
       "2007  0.015630  0.041839 -1.078481  ... -0.033619 -0.082491 -0.015605   \n",
       "2008  0.045812  0.124574 -0.915449  ... -0.057947 -0.107110 -0.059751   \n",
       "2009  0.027495  0.021102 -1.011296  ... -0.004887 -0.040088  0.006978   \n",
       "\n",
       "           293       294       295       296       297       298       299  \n",
       "0     0.004901 -0.032685 -0.012092  0.049251 -0.013650 -0.049144  0.040632  \n",
       "1    -0.021560  0.020954 -0.086438 -0.016503 -0.022217 -0.050941  0.033541  \n",
       "2     0.038621  0.006540  0.090790 -0.031528 -0.063819 -0.022391  0.042568  \n",
       "3     0.046466 -0.054474 -0.075757  0.024841 -0.043944  0.007172  0.068454  \n",
       "4    -0.017479  0.027504 -0.106758 -0.026807 -0.033834 -0.049099  0.047046  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "2005 -0.032949  0.077111 -0.101313 -0.003604 -0.073269 -0.119029  0.070678  \n",
       "2006 -0.028961 -0.026986 -0.113882  0.053097 -0.047088 -0.024108  0.029061  \n",
       "2007 -0.047796 -0.018306  0.026556 -0.104593 -0.058046 -0.049240  0.011466  \n",
       "2008  0.042056  0.046700 -0.087650  0.009637 -0.017887 -0.061365  0.116531  \n",
       "2009  0.005945  0.031613 -0.037447 -0.002787 -0.014828 -0.054768  0.074454  \n",
       "\n",
       "[2010 rows x 300 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(train_features_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2e986f2e-a1f6-4d27-9853-9ddea7d16e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.085171</td>\n",
       "      <td>0.087697</td>\n",
       "      <td>-0.034940</td>\n",
       "      <td>-0.064675</td>\n",
       "      <td>0.031153</td>\n",
       "      <td>0.007324</td>\n",
       "      <td>-0.096535</td>\n",
       "      <td>0.016857</td>\n",
       "      <td>0.015636</td>\n",
       "      <td>-1.194943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002860</td>\n",
       "      <td>-0.071951</td>\n",
       "      <td>-0.111008</td>\n",
       "      <td>0.018815</td>\n",
       "      <td>-0.020504</td>\n",
       "      <td>-0.046612</td>\n",
       "      <td>-0.031983</td>\n",
       "      <td>-0.001246</td>\n",
       "      <td>-0.002914</td>\n",
       "      <td>-0.042785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.071061</td>\n",
       "      <td>0.031449</td>\n",
       "      <td>-0.086494</td>\n",
       "      <td>-0.105086</td>\n",
       "      <td>0.026560</td>\n",
       "      <td>0.076512</td>\n",
       "      <td>-0.035002</td>\n",
       "      <td>0.067925</td>\n",
       "      <td>-0.024546</td>\n",
       "      <td>-0.882894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008911</td>\n",
       "      <td>-0.040386</td>\n",
       "      <td>-0.118541</td>\n",
       "      <td>-0.027533</td>\n",
       "      <td>0.020789</td>\n",
       "      <td>-0.186462</td>\n",
       "      <td>0.032365</td>\n",
       "      <td>0.010867</td>\n",
       "      <td>0.048979</td>\n",
       "      <td>0.102556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.055935</td>\n",
       "      <td>0.031655</td>\n",
       "      <td>0.041022</td>\n",
       "      <td>-0.011393</td>\n",
       "      <td>0.079114</td>\n",
       "      <td>0.136252</td>\n",
       "      <td>-0.014718</td>\n",
       "      <td>-0.029704</td>\n",
       "      <td>-0.008770</td>\n",
       "      <td>-0.865452</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003628</td>\n",
       "      <td>-0.031124</td>\n",
       "      <td>0.046586</td>\n",
       "      <td>-0.017082</td>\n",
       "      <td>0.002051</td>\n",
       "      <td>-0.117929</td>\n",
       "      <td>-0.025070</td>\n",
       "      <td>0.002550</td>\n",
       "      <td>-0.049759</td>\n",
       "      <td>0.040466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.117528</td>\n",
       "      <td>0.048562</td>\n",
       "      <td>0.018459</td>\n",
       "      <td>-0.099897</td>\n",
       "      <td>0.036499</td>\n",
       "      <td>0.097177</td>\n",
       "      <td>-0.015837</td>\n",
       "      <td>0.013810</td>\n",
       "      <td>0.006456</td>\n",
       "      <td>-0.792350</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018390</td>\n",
       "      <td>-0.078470</td>\n",
       "      <td>-0.017232</td>\n",
       "      <td>-0.014702</td>\n",
       "      <td>-0.044294</td>\n",
       "      <td>-0.148532</td>\n",
       "      <td>-0.028399</td>\n",
       "      <td>-0.041426</td>\n",
       "      <td>-0.027574</td>\n",
       "      <td>0.052501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.056573</td>\n",
       "      <td>0.014105</td>\n",
       "      <td>-0.042295</td>\n",
       "      <td>0.026413</td>\n",
       "      <td>-0.037524</td>\n",
       "      <td>0.035594</td>\n",
       "      <td>-0.005434</td>\n",
       "      <td>0.069709</td>\n",
       "      <td>0.011840</td>\n",
       "      <td>-0.904231</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058551</td>\n",
       "      <td>-0.110081</td>\n",
       "      <td>0.005421</td>\n",
       "      <td>-0.024142</td>\n",
       "      <td>-0.031935</td>\n",
       "      <td>-0.264091</td>\n",
       "      <td>-0.023653</td>\n",
       "      <td>0.087820</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.069631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>-0.051394</td>\n",
       "      <td>0.035845</td>\n",
       "      <td>-0.034806</td>\n",
       "      <td>-0.065552</td>\n",
       "      <td>0.032538</td>\n",
       "      <td>0.143753</td>\n",
       "      <td>-0.056050</td>\n",
       "      <td>-0.000659</td>\n",
       "      <td>0.059859</td>\n",
       "      <td>-0.692159</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035126</td>\n",
       "      <td>-0.020249</td>\n",
       "      <td>-0.036673</td>\n",
       "      <td>0.016049</td>\n",
       "      <td>-0.021832</td>\n",
       "      <td>-0.073260</td>\n",
       "      <td>-0.008150</td>\n",
       "      <td>-0.003551</td>\n",
       "      <td>0.012078</td>\n",
       "      <td>-0.043570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>-0.056376</td>\n",
       "      <td>0.074387</td>\n",
       "      <td>0.004264</td>\n",
       "      <td>-0.162391</td>\n",
       "      <td>-0.075184</td>\n",
       "      <td>0.091519</td>\n",
       "      <td>-0.046987</td>\n",
       "      <td>-0.005608</td>\n",
       "      <td>0.001129</td>\n",
       "      <td>-0.900835</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.099467</td>\n",
       "      <td>-0.093213</td>\n",
       "      <td>-0.068575</td>\n",
       "      <td>0.094120</td>\n",
       "      <td>0.041656</td>\n",
       "      <td>-0.026783</td>\n",
       "      <td>0.066266</td>\n",
       "      <td>0.006180</td>\n",
       "      <td>0.028854</td>\n",
       "      <td>-0.009676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>-0.066837</td>\n",
       "      <td>0.010423</td>\n",
       "      <td>-0.056334</td>\n",
       "      <td>-0.006306</td>\n",
       "      <td>-0.001397</td>\n",
       "      <td>0.062324</td>\n",
       "      <td>-0.127318</td>\n",
       "      <td>-0.037340</td>\n",
       "      <td>-0.076692</td>\n",
       "      <td>-0.680659</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075357</td>\n",
       "      <td>0.024648</td>\n",
       "      <td>0.014440</td>\n",
       "      <td>-0.015972</td>\n",
       "      <td>-0.074291</td>\n",
       "      <td>-0.091538</td>\n",
       "      <td>-0.064304</td>\n",
       "      <td>0.054602</td>\n",
       "      <td>0.023180</td>\n",
       "      <td>0.097109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>-0.075133</td>\n",
       "      <td>-0.013684</td>\n",
       "      <td>0.072210</td>\n",
       "      <td>-0.046680</td>\n",
       "      <td>-0.016196</td>\n",
       "      <td>0.043928</td>\n",
       "      <td>-0.042347</td>\n",
       "      <td>-0.035546</td>\n",
       "      <td>-0.074855</td>\n",
       "      <td>-0.701632</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059805</td>\n",
       "      <td>-0.026380</td>\n",
       "      <td>0.032301</td>\n",
       "      <td>-0.064150</td>\n",
       "      <td>-0.108212</td>\n",
       "      <td>-0.140116</td>\n",
       "      <td>-0.037404</td>\n",
       "      <td>-0.028904</td>\n",
       "      <td>-0.002801</td>\n",
       "      <td>0.085904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>-0.087517</td>\n",
       "      <td>-0.007487</td>\n",
       "      <td>-0.038844</td>\n",
       "      <td>-0.041629</td>\n",
       "      <td>0.022613</td>\n",
       "      <td>0.045465</td>\n",
       "      <td>-0.007900</td>\n",
       "      <td>-0.013784</td>\n",
       "      <td>0.071971</td>\n",
       "      <td>-0.957573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012488</td>\n",
       "      <td>-0.024468</td>\n",
       "      <td>0.059532</td>\n",
       "      <td>0.038480</td>\n",
       "      <td>0.019515</td>\n",
       "      <td>-0.134994</td>\n",
       "      <td>0.035589</td>\n",
       "      <td>-0.002156</td>\n",
       "      <td>-0.030536</td>\n",
       "      <td>0.092054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>990 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "0   -0.085171  0.087697 -0.034940 -0.064675  0.031153  0.007324 -0.096535   \n",
       "1   -0.071061  0.031449 -0.086494 -0.105086  0.026560  0.076512 -0.035002   \n",
       "2   -0.055935  0.031655  0.041022 -0.011393  0.079114  0.136252 -0.014718   \n",
       "3   -0.117528  0.048562  0.018459 -0.099897  0.036499  0.097177 -0.015837   \n",
       "4   -0.056573  0.014105 -0.042295  0.026413 -0.037524  0.035594 -0.005434   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "985 -0.051394  0.035845 -0.034806 -0.065552  0.032538  0.143753 -0.056050   \n",
       "986 -0.056376  0.074387  0.004264 -0.162391 -0.075184  0.091519 -0.046987   \n",
       "987 -0.066837  0.010423 -0.056334 -0.006306 -0.001397  0.062324 -0.127318   \n",
       "988 -0.075133 -0.013684  0.072210 -0.046680 -0.016196  0.043928 -0.042347   \n",
       "989 -0.087517 -0.007487 -0.038844 -0.041629  0.022613  0.045465 -0.007900   \n",
       "\n",
       "          7         8         9    ...       290       291       292  \\\n",
       "0    0.016857  0.015636 -1.194943  ...  0.002860 -0.071951 -0.111008   \n",
       "1    0.067925 -0.024546 -0.882894  ...  0.008911 -0.040386 -0.118541   \n",
       "2   -0.029704 -0.008770 -0.865452  ...  0.003628 -0.031124  0.046586   \n",
       "3    0.013810  0.006456 -0.792350  ... -0.018390 -0.078470 -0.017232   \n",
       "4    0.069709  0.011840 -0.904231  ... -0.058551 -0.110081  0.005421   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "985 -0.000659  0.059859 -0.692159  ... -0.035126 -0.020249 -0.036673   \n",
       "986 -0.005608  0.001129 -0.900835  ... -0.099467 -0.093213 -0.068575   \n",
       "987 -0.037340 -0.076692 -0.680659  ... -0.075357  0.024648  0.014440   \n",
       "988 -0.035546 -0.074855 -0.701632  ...  0.059805 -0.026380  0.032301   \n",
       "989 -0.013784  0.071971 -0.957573  ...  0.012488 -0.024468  0.059532   \n",
       "\n",
       "          293       294       295       296       297       298       299  \n",
       "0    0.018815 -0.020504 -0.046612 -0.031983 -0.001246 -0.002914 -0.042785  \n",
       "1   -0.027533  0.020789 -0.186462  0.032365  0.010867  0.048979  0.102556  \n",
       "2   -0.017082  0.002051 -0.117929 -0.025070  0.002550 -0.049759  0.040466  \n",
       "3   -0.014702 -0.044294 -0.148532 -0.028399 -0.041426 -0.027574  0.052501  \n",
       "4   -0.024142 -0.031935 -0.264091 -0.023653  0.087820  0.000125  0.069631  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "985  0.016049 -0.021832 -0.073260 -0.008150 -0.003551  0.012078 -0.043570  \n",
       "986  0.094120  0.041656 -0.026783  0.066266  0.006180  0.028854 -0.009676  \n",
       "987 -0.015972 -0.074291 -0.091538 -0.064304  0.054602  0.023180  0.097109  \n",
       "988 -0.064150 -0.108212 -0.140116 -0.037404 -0.028904 -0.002801  0.085904  \n",
       "989  0.038480  0.019515 -0.134994  0.035589 -0.002156 -0.030536  0.092054  \n",
       "\n",
       "[990 rows x 300 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(test_features_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce60d31-039b-4fdd-bf68-d89f39edd376",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "356d08a7-66d1-4cd7-bbf6-6a00a3cb0f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "75e28382-c5d8-4aa5-85a4-70af9d36d135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_features_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bc8bf7ff-a970-41d5-934f-1c946cef8b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = model.predict(train_features_X)\n",
    "test_pred = model.predict(test_features_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f86f59a7-5b78-483a-816b-d6807d1ef7ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[860, 149],\n",
       "       [144, 857]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(train_y, train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "38c081da-f486-411a-885c-15c0f5967cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[405,  88],\n",
       "       [ 98, 399]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_y, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "20f7427e-408c-4036-85ec-cba47a111d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy :  0.8542288557213931\n",
      "Test Accuracy :  0.8121212121212121\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Accuracy : \",accuracy_score(train_y, train_pred))\n",
    "print(\"Test Accuracy : \",accuracy_score(test_y, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a997079b-34f2-4371-b035-05897ebc201a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9df48d73-6171-4c0f-9e53-b42421a96d61",
   "metadata": {},
   "source": [
    "# Glove with Deep Learning Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "89cd1256-3b53-4e62-82f9-a7f7f131d651",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation,Dropout, BatchNormalization\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "12b9e082-69fd-4d43-8c24-a1224ea1aee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat_train = to_categorical(np.asarray(train_y.factorize()[0]))\n",
    "y_cat_test = to_categorical(np.asarray(test_y.factorize()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "beb9eb46-5b4c-467a-aecc-200c9d71e0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/keras/src/layers/core/dense.py:85: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, kernel_initializer = 'he_normal', input_shape = (300,),activation = 'elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, kernel_initializer = 'he_normal', activation = 'elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(16 ,kernel_initializer = 'he_normal',activation = 'elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(8 ,kernel_initializer = 'he_normal', activation = 'elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(4 ,kernel_initializer = 'he_normal', activation = 'elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, kernel_initializer = 'he_normal', activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "c5517e5c-b044-4d23-b833-f727a3177c88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_11\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_11\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_66 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">19,264</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_55 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_67 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_56 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_68 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_12          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_57 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_69 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_13          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_58 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_70 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_14          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_59 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_71 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_66 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m19,264\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_55 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_67 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_56 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_68 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_12          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_57 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_69 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │           \u001b[38;5;34m136\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_13          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │            \u001b[38;5;34m32\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_58 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_70 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │            \u001b[38;5;34m36\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_14          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │            \u001b[38;5;34m16\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_59 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_71 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │            \u001b[38;5;34m10\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">22,550</span> (88.09 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m22,550\u001b[0m (88.09 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">22,302</span> (87.12 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m22,302\u001b[0m (87.12 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">248</span> (992.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m248\u001b[0m (992.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a4d404d1-0763-460e-9f3d-bb0bc8014ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "25236400-0faa-41af-8cf8-513863163542",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5470 - loss: 0.7953\n",
      "Epoch 2/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6795 - loss: 0.6728\n",
      "Epoch 3/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6990 - loss: 0.6245\n",
      "Epoch 4/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7122 - loss: 0.6145\n",
      "Epoch 5/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7430 - loss: 0.5847\n",
      "Epoch 6/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7632 - loss: 0.5675\n",
      "Epoch 7/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7766 - loss: 0.5400\n",
      "Epoch 8/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7965 - loss: 0.5273\n",
      "Epoch 9/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7743 - loss: 0.5188\n",
      "Epoch 10/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 976us/step - accuracy: 0.7994 - loss: 0.4931\n",
      "Epoch 11/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 941us/step - accuracy: 0.8091 - loss: 0.4842\n",
      "Epoch 12/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 964us/step - accuracy: 0.8092 - loss: 0.4671\n",
      "Epoch 13/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 897us/step - accuracy: 0.8346 - loss: 0.4444\n",
      "Epoch 14/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 880us/step - accuracy: 0.8234 - loss: 0.4457\n",
      "Epoch 15/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 908us/step - accuracy: 0.8172 - loss: 0.4412\n",
      "Epoch 16/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 843us/step - accuracy: 0.8397 - loss: 0.4253\n",
      "Epoch 17/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 877us/step - accuracy: 0.8396 - loss: 0.4169\n",
      "Epoch 18/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 801us/step - accuracy: 0.8439 - loss: 0.3957\n",
      "Epoch 19/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - accuracy: 0.8496 - loss: 0.4071\n",
      "Epoch 20/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 776us/step - accuracy: 0.8627 - loss: 0.3826\n",
      "Epoch 21/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 849us/step - accuracy: 0.8734 - loss: 0.3692\n",
      "Epoch 22/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 847us/step - accuracy: 0.8609 - loss: 0.3788\n",
      "Epoch 23/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 841us/step - accuracy: 0.8545 - loss: 0.3870\n",
      "Epoch 24/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822us/step - accuracy: 0.8610 - loss: 0.3734\n",
      "Epoch 25/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 934us/step - accuracy: 0.8622 - loss: 0.3655\n",
      "Epoch 26/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 843us/step - accuracy: 0.8727 - loss: 0.3449\n",
      "Epoch 27/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - accuracy: 0.8721 - loss: 0.3545\n",
      "Epoch 28/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875us/step - accuracy: 0.8706 - loss: 0.3566\n",
      "Epoch 29/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 876us/step - accuracy: 0.8695 - loss: 0.3561\n",
      "Epoch 30/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 855us/step - accuracy: 0.8739 - loss: 0.3360\n",
      "Epoch 31/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - accuracy: 0.8703 - loss: 0.3332\n",
      "Epoch 32/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 856us/step - accuracy: 0.8786 - loss: 0.3155\n",
      "Epoch 33/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step - accuracy: 0.8871 - loss: 0.3168\n",
      "Epoch 34/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - accuracy: 0.8723 - loss: 0.3477\n",
      "Epoch 35/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 798us/step - accuracy: 0.8816 - loss: 0.3200\n",
      "Epoch 36/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 766us/step - accuracy: 0.8865 - loss: 0.2978\n",
      "Epoch 37/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - accuracy: 0.8786 - loss: 0.3179\n",
      "Epoch 38/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 888us/step - accuracy: 0.8760 - loss: 0.3415\n",
      "Epoch 39/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 823us/step - accuracy: 0.8857 - loss: 0.2916\n",
      "Epoch 40/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875us/step - accuracy: 0.8938 - loss: 0.2985\n",
      "Epoch 41/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 889us/step - accuracy: 0.8760 - loss: 0.3218\n",
      "Epoch 42/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 815us/step - accuracy: 0.8941 - loss: 0.2774\n",
      "Epoch 43/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867us/step - accuracy: 0.8994 - loss: 0.2774\n",
      "Epoch 44/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 859us/step - accuracy: 0.8889 - loss: 0.2909\n",
      "Epoch 45/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - accuracy: 0.8934 - loss: 0.2890\n",
      "Epoch 46/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 851us/step - accuracy: 0.8972 - loss: 0.2727\n",
      "Epoch 47/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 808us/step - accuracy: 0.9134 - loss: 0.2612\n",
      "Epoch 48/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - accuracy: 0.9075 - loss: 0.2616\n",
      "Epoch 49/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875us/step - accuracy: 0.9044 - loss: 0.2781\n",
      "Epoch 50/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 853us/step - accuracy: 0.9016 - loss: 0.2595\n",
      "Epoch 51/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 848us/step - accuracy: 0.9085 - loss: 0.2489\n",
      "Epoch 52/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 821us/step - accuracy: 0.9066 - loss: 0.2497\n",
      "Epoch 53/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - accuracy: 0.9056 - loss: 0.2661\n",
      "Epoch 54/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9042 - loss: 0.2604\n",
      "Epoch 55/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 962us/step - accuracy: 0.8937 - loss: 0.2710\n",
      "Epoch 56/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922us/step - accuracy: 0.9110 - loss: 0.2490\n",
      "Epoch 57/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 947us/step - accuracy: 0.8957 - loss: 0.2674\n",
      "Epoch 58/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - accuracy: 0.9322 - loss: 0.2155\n",
      "Epoch 59/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 917us/step - accuracy: 0.9179 - loss: 0.2382\n",
      "Epoch 60/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 949us/step - accuracy: 0.9143 - loss: 0.2409\n",
      "Epoch 61/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 920us/step - accuracy: 0.8951 - loss: 0.2698\n",
      "Epoch 62/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 817us/step - accuracy: 0.9292 - loss: 0.2303\n",
      "Epoch 63/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 877us/step - accuracy: 0.9284 - loss: 0.2041\n",
      "Epoch 64/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 872us/step - accuracy: 0.9198 - loss: 0.2277\n",
      "Epoch 65/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 903us/step - accuracy: 0.9248 - loss: 0.2221\n",
      "Epoch 66/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 927us/step - accuracy: 0.9097 - loss: 0.2318\n",
      "Epoch 67/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 999us/step - accuracy: 0.9354 - loss: 0.2026\n",
      "Epoch 68/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9374 - loss: 0.2030\n",
      "Epoch 69/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 925us/step - accuracy: 0.9116 - loss: 0.2409\n",
      "Epoch 70/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 942us/step - accuracy: 0.9237 - loss: 0.2175\n",
      "Epoch 71/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 915us/step - accuracy: 0.9215 - loss: 0.2155\n",
      "Epoch 72/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 943us/step - accuracy: 0.9249 - loss: 0.1962\n",
      "Epoch 73/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 909us/step - accuracy: 0.9316 - loss: 0.1971\n",
      "Epoch 74/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 904us/step - accuracy: 0.9321 - loss: 0.2025\n",
      "Epoch 75/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 868us/step - accuracy: 0.9255 - loss: 0.2083\n",
      "Epoch 76/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 898us/step - accuracy: 0.9370 - loss: 0.1846\n",
      "Epoch 77/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 878us/step - accuracy: 0.9344 - loss: 0.2058\n",
      "Epoch 78/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 903us/step - accuracy: 0.9216 - loss: 0.2106\n",
      "Epoch 79/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - accuracy: 0.9311 - loss: 0.2019\n",
      "Epoch 80/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 946us/step - accuracy: 0.9284 - loss: 0.2079\n",
      "Epoch 81/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9315 - loss: 0.2172\n",
      "Epoch 82/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9350 - loss: 0.1974 \n",
      "Epoch 83/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9447 - loss: 0.1734\n",
      "Epoch 84/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9447 - loss: 0.1790\n",
      "Epoch 85/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9361 - loss: 0.1866\n",
      "Epoch 86/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 976us/step - accuracy: 0.9268 - loss: 0.2107\n",
      "Epoch 87/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9395 - loss: 0.1946\n",
      "Epoch 88/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 996us/step - accuracy: 0.9462 - loss: 0.1577\n",
      "Epoch 89/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 975us/step - accuracy: 0.9416 - loss: 0.1773\n",
      "Epoch 90/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9353 - loss: 0.1856\n",
      "Epoch 91/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9509 - loss: 0.1638\n",
      "Epoch 92/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 991us/step - accuracy: 0.9434 - loss: 0.1651\n",
      "Epoch 93/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 990us/step - accuracy: 0.9503 - loss: 0.1574\n",
      "Epoch 94/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 985us/step - accuracy: 0.9434 - loss: 0.1607\n",
      "Epoch 95/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 928us/step - accuracy: 0.9456 - loss: 0.1679\n",
      "Epoch 96/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 819us/step - accuracy: 0.9398 - loss: 0.1766\n",
      "Epoch 97/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 863us/step - accuracy: 0.9551 - loss: 0.1466\n",
      "Epoch 98/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 865us/step - accuracy: 0.9623 - loss: 0.1371\n",
      "Epoch 99/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 850us/step - accuracy: 0.9536 - loss: 0.1553\n",
      "Epoch 100/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - accuracy: 0.9565 - loss: 0.1418\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_features_X,y_cat_train,epochs = 100, verbose = 1, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "fe85887e-1061-4fec-bf70-965f37a3b761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374us/step\n"
     ]
    }
   ],
   "source": [
    "train_pred = np.argmax(model.predict(train_features_X), axis = 1)\n",
    "test_pred = np.argmax(model.predict(test_features_X), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "a527e1ce-4401-4078-b3f1-6626ef9c9ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = np.where(train_pred ==0, 'positive', 'negative')\n",
    "test_pred = np.where(test_pred ==0, 'positive', 'negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "e9f6bded-505d-4fd5-a027-d4bc733666b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy :  0.9805970149253731\n",
      "Test Accuracy :  0.8010101010101011\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Accuracy : \",accuracy_score(train_y, train_pred))\n",
    "print(\"Test Accuracy : \",accuracy_score(test_y, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65a343e-cbd7-4491-b8f7-c16742c21dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259db642-a141-4196-8640-2e569cb1e803",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Word2vec Embedding - Google New Vector\n",
    "2. Train Word2vec Embedding\n",
    "3. Train FastText Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "6dac39bd-a995-49c3-8b4a-07ed9aa081c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "af5b2eff-6915-49b0-8b0f-ef56844fcddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FastText Embedding Vector Download\n",
    "#https://fasttext.cc/docs/en/english-vectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb5401c-8267-4bfa-938d-851f0df698d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
